{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load as LUME-model\n",
    "\n",
    "Loads the base neural network model, the `sim_to_nn` transformers and the simulation variable specification from their respective files to create a [LUME-model](https://github.com/slaclab/lume-model/). The resulting instance of `TorchModel` enforces requirements on the input and output variables and can be wrapped in a `TorchModule`. The `TorchModule` can be used like a `torch.nn.Module` and is tested on a small set of simulation data.\n",
    "\n",
    "Here we show how to register and update this model with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T12:20:23.809750Z",
     "start_time": "2023-04-26T12:20:21.653781Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lume_model.utils import variables_from_yaml\n",
    "from lume_model.models import TorchModel, TorchModule\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\")  \n",
    "#os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://127.0.0.1:8082\" # for local testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/742261476526561220', creation_time=1752699169877, experiment_id='742261476526561220', last_update_time=1752699169877, lifecycle_stage='active', name='lcls-injector-ML', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"lcls-injector-ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no active run is lingering\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model as TorchModel and TorchModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PyTorch model from file: ../model/model.pt\n"
     ]
    }
   ],
   "source": [
    "# load transformers\n",
    "input_sim_to_nn = torch.load(\"../model/input_sim_to_nn.pt\", weights_only=False)\n",
    "output_sim_to_nn = torch.load(\"../model/output_sim_to_nn.pt\", weights_only=False)\n",
    "# load in- and output variable specification\n",
    "input_variables, output_variables = variables_from_yaml(\"../model/sim_variables.yml\")\n",
    "\n",
    "# create TorchModel\n",
    "lume_model = TorchModel(\n",
    "    model=\"../model/model.pt\",\n",
    "    input_variables=input_variables,\n",
    "    output_variables=output_variables,\n",
    "    input_transformers=[input_sim_to_nn],\n",
    "    output_transformers=[output_sim_to_nn],\n",
    ")\n",
    "\n",
    "# wrap in TorchModule\n",
    "lume_module = TorchModule(\n",
    "    model=lume_model,\n",
    "    input_order=lume_model.input_names,\n",
    "    output_order=lume_model.output_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load example data that we need for mlflow\n",
    "inputs_small = torch.load(\"../info/inputs_small.pt\")\n",
    "outputs_small = torch.load(\"../info/outputs_small.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register model to MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See function signature for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlume_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_to_mlflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0martifact_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mregistered_model_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mversion_tags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0malias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrun_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlog_model_dump\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msave_jit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Registers the model to MLflow if mlflow is installed. Each time this function is called, a new version\n",
       "of the model is created. The model is saved to the tracking server or local directory, depending on the\n",
       "MLFLOW_TRACKING_URI.\n",
       "\n",
       "If no tracking server is set up, data and artifacts are saved directly under your current directory. To set up\n",
       "a tracking server, set the environment variable MLFLOW_TRACKING_URI, e.g. a local port/path. See\n",
       "https://mlflow.org/docs/latest/getting-started/intro-quickstart/ for more info.\n",
       "\n",
       "Args:\n",
       "    artifact_path: Path to store the model in MLflow.\n",
       "    registered_model_name: Name of the registered model in MLflow. Optional.\n",
       "    tags: Tags to add to the MLflow model. Optional.\n",
       "    version_tags: Tags to add to this MLflow model version. Optional.\n",
       "    alias: Alias to add to this MLflow model version. Optional.\n",
       "    run_name: Name of the MLflow run. Optional.\n",
       "    log_model_dump: Whether to log the model dump files as artifacts. Optional.\n",
       "    save_jit: Whether to save the model as TorchScript when calling model.dump, if log_model_dump=True. Optional.\n",
       "    **kwargs: Additional arguments for mlflow.pyfunc.log_model.\n",
       "\n",
       "Returns:\n",
       "    Model info metadata, mlflow.models.model.ModelInfo.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/SLAC/lume-model/lume_model/models/torch_module.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lume_module.register_to_mlflow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run lume-test at: http://127.0.0.1:8082/#/experiments/742261476526561220/runs/ce536dd7f16e48e69da0bd5ae8f20dbd\n",
      "üß™ View experiment at: http://127.0.0.1:8082/#/experiments/742261476526561220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'lcls_injector_torch_module'.\n",
      "Created version '1' of model 'lcls_injector_torch_module'.\n"
     ]
    }
   ],
   "source": [
    "model_info = lume_module.register_to_mlflow(\n",
    "            artifact_path=\"lcls_injector_torch_module\",\n",
    "            registered_model_name=\"lcls_injector_torch_module\", # not necessary but required for adding tags/aliases\n",
    "            tags={\"type\":\"surrogate_injector\"}, # example tag, if desired\n",
    "            run_name=\"lume-test\" # will be generated randomly if not provided\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calling `lume_module.register_to_mlflow` again with the same `registered_model_name`, the model version will be incremented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc60e7a295c47618ca06d8a09957540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# original model\n",
    "with torch.no_grad():\n",
    "    predictions = lume_module(inputs_small)\n",
    "\n",
    "# Load using version we just registered to MLflow\n",
    "version = model_info.registered_model_version\n",
    "model_saved = f\"models:/lcls_injector_torch_module/{version}\"\n",
    "model_saved = mlflow.pytorch.load_model(model_saved)\n",
    "predictions_load = model_saved(inputs_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (predictions == predictions_load).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log performance metric\n",
    "mae = torch.mean(torch.abs(predictions - outputs_small)).item()\n",
    "\n",
    "# Plot and save\n",
    "nrows, ncols = 3, 2\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 15))\n",
    "for i, output_name in enumerate(lume_module.output_order):\n",
    "    ax_i = ax[i // ncols, i % ncols]\n",
    "    if i < outputs_small.shape[1]:\n",
    "        sort_idx = torch.argsort(outputs_small[:, i])\n",
    "        x_axis = torch.arange(outputs_small.shape[0])\n",
    "        ax_i.plot(x_axis, outputs_small[sort_idx, i], \"C0x\", label=\"outputs\")\n",
    "        ax_i.plot(x_axis, predictions[sort_idx, i], \"C1x\", label=\"predictions\")\n",
    "        ax_i.legend()\n",
    "        ax_i.set_title(output_name)\n",
    "ax[-1, -1].axis('off')\n",
    "fig.tight_layout()\n",
    "\n",
    "plot_path = \"comparison_plot_lume.png\"\n",
    "plt.savefig(plot_path)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `lume_module.register_to_mlflow` ends the run automatically, but if you'd like to go back and update it, e.g. log an artifact, you can do so as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run lume-test at: http://127.0.0.1:8082/#/experiments/742261476526561220/runs/ce536dd7f16e48e69da0bd5ae8f20dbd\n",
      "üß™ View experiment at: http://127.0.0.1:8082/#/experiments/742261476526561220\n"
     ]
    }
   ],
   "source": [
    "run_id = model_info.run_id\n",
    "with mlflow.start_run(run_id=run_id) as run:\n",
    "    # log some metric\n",
    "    mlflow.log_metric(\"mean_absolute_error\", mae)\n",
    "    # log the image file\n",
    "    mlflow.log_artifact(plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
