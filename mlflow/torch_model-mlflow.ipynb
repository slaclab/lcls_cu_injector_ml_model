{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load as torch model\n",
    "\n",
    "Loads the base neural network model and the `sim_to_nn` transformers from their respective files. They are combined to create a `TransformedModel` and tested on a small set of simulation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://mlflow-bucket/5', creation_time=1746036982208, experiment_id='5', last_update_time=1746036982208, lifecycle_stage='active', name='lcls-injector-ML', tags={}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])\n",
    "mlflow.set_experiment(\"lcls-injector-ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model and transformers\n",
    "model = torch.load(\"../model/model.pt\")\n",
    "input_sim_to_nn = torch.load(\"../model/input_sim_to_nn.pt\")\n",
    "output_sim_to_nn = torch.load(\"../model/output_sim_to_nn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformed model class\n",
    "class TransformedModel(torch.nn.Module):\n",
    "    def __init__(self, model, input_transformer, output_transformer):\n",
    "        super(TransformedModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.input_transformer = input_transformer\n",
    "        self.output_transformer = output_transformer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_transformer(x)\n",
    "        x = self.model(x)\n",
    "        x = self.output_transformer.untransform(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformedModel(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=100, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Linear(in_features=100, out_features=200, bias=True)\n",
       "    (3): ELU(alpha=1.0)\n",
       "    (4): Dropout(p=0.05, inplace=False)\n",
       "    (5): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (6): ELU(alpha=1.0)\n",
       "    (7): Dropout(p=0.05, inplace=False)\n",
       "    (8): Linear(in_features=200, out_features=300, bias=True)\n",
       "    (9): ELU(alpha=1.0)\n",
       "    (10): Dropout(p=0.05, inplace=False)\n",
       "    (11): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (12): ELU(alpha=1.0)\n",
       "    (13): Dropout(p=0.05, inplace=False)\n",
       "    (14): Linear(in_features=300, out_features=200, bias=True)\n",
       "    (15): ELU(alpha=1.0)\n",
       "    (16): Dropout(p=0.05, inplace=False)\n",
       "    (17): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (18): ELU(alpha=1.0)\n",
       "    (19): Dropout(p=0.05, inplace=False)\n",
       "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (21): ELU(alpha=1.0)\n",
       "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (23): ELU(alpha=1.0)\n",
       "    (24): Linear(in_features=100, out_features=5, bias=True)\n",
       "  )\n",
       "  (input_transformer): AffineInputTransform()\n",
       "  (output_transformer): AffineInputTransform()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create transformed model\n",
    "transformed_model = TransformedModel(\n",
    "    model=model, \n",
    "    input_transformer=input_sim_to_nn,\n",
    "    output_transformer=output_sim_to_nn,\n",
    ").to(torch.double)\n",
    "transformed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/02 10:03:42 INFO mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics. Set logger level to DEBUG for more details.\n",
      "2025/05/02 10:03:42 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
      "\u001b[31m2025/05/02 10:05:03 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run lcls-injector-ML Run6 at: https://ard-mlflow.slac.stanford.edu/#/experiments/5/runs/8efaa5dc63b04bf2831d2ac7a985b9e2\n",
      "üß™ View experiment at: https://ard-mlflow.slac.stanford.edu/#/experiments/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/02 10:05:15 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2025/05/02 10:05:16 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    }
   ],
   "source": [
    "# load example data and calculate predictions\n",
    "import torch.nn.functional as F\n",
    "\n",
    "with mlflow.start_run(run_name=\"lcls-injector-ML Run6\"):\n",
    "    mlflow.log_param(\"model_type\", str(type(model)))\n",
    "    mlflow.log_param(\"input_transformer_type\", str(type(input_sim_to_nn)))\n",
    "    mlflow.log_param(\"output_transformer_type\", str(type(output_sim_to_nn)))\n",
    "    \n",
    "    # Load input/output data\n",
    "    inputs_small = torch.load(\"../info/inputs_small.pt\")\n",
    "    outputs_small = torch.load(\"../info/outputs_small.pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = transformed_model(inputs_small)\n",
    "\n",
    "    mae = torch.mean(torch.abs(predictions - outputs_small)).item()\n",
    "    mlflow.log_metric(\"mean_absolute_error\", mae)\n",
    "    mlflow.pytorch.log_model(transformed_model, artifact_path=\"transformed_model\")\n",
    "\n",
    "    # Plot predictions vs ground truth\n",
    "    nrows, ncols = 3, 2\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 15))\n",
    "    for i in range(nrows * ncols):\n",
    "        ax_i = ax[i // ncols, i % ncols]\n",
    "        if i < outputs_small.shape[1]:\n",
    "            sort_idx = torch.argsort(outputs_small[:, i])\n",
    "            x_axis = torch.arange(outputs_small.shape[0])\n",
    "            ax_i.plot(x_axis, outputs_small[sort_idx, i], \"C0x\", label=\"outputs\")\n",
    "            ax_i.plot(x_axis, predictions[sort_idx, i], \"C1x\", label=\"predictions\")\n",
    "            ax_i.legend()\n",
    "    ax[-1, -1].axis('off')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save and log plot\n",
    "    plot_path = \"comparison_plot.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run mercurial-horse-202 at: https://ard-mlflow.slac.stanford.edu/#/experiments/5/runs/75f39c7e5c8441299256edb716d21717\n",
      "üß™ View experiment at: https://ard-mlflow.slac.stanford.edu/#/experiments/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/30 11:31:16 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2025/04/30 11:31:16 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "if mlflow.active_run() is not None:\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
