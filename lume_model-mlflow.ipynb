{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load as LUME-model\n",
    "\n",
    "Loads the base neural network model, the `sim_to_nn` transformers and the simulation variable specification from their respective files to create a [LUME-model](https://github.com/slaclab/lume-model/). The resulting instance of `TorchModel` enforces requirements on the input and output variables and can be wrapped in a `TorchModule`. The `TorchModule` can be used like a `torch.nn.Module` and is tested on a small set of simulation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T12:20:23.809750Z",
     "start_time": "2023-04-26T12:20:21.653781Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lume_model.utils import variables_from_yaml\n",
    "from lume_model.models import TorchModel, TorchModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://mlflow-bucket/5', creation_time=1746036982208, experiment_id='5', last_update_time=1746036982208, lifecycle_stage='active', name='lcls-injector-ML', tags={}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])\n",
    "mlflow.set_experiment(\"lcls-injector-ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no active run is lingering\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/30 14:08:29 INFO mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics. Set logger level to DEBUG for more details.\n",
      "2025/04/30 14:08:29 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PyTorch model from file: model/model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/30 14:08:32 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run LumeModel Run at: https://ard-mlflow.slac.stanford.edu/#/experiments/5/runs/4937a5c7f598491c92f1b8bb627802d6\n",
      "üß™ View experiment at: https://ard-mlflow.slac.stanford.edu/#/experiments/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/30 14:08:32 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with mlflow.start_run(run_name=\"LumeModel Run\"):\n",
    "    # load transformers\n",
    "    input_sim_to_nn = torch.load(\"model/input_sim_to_nn.pt\")\n",
    "    output_sim_to_nn = torch.load(\"model/output_sim_to_nn.pt\")\n",
    "    # load in- and output variable specification\n",
    "    input_variables, output_variables = variables_from_yaml(\"model/sim_variables.yml\")\n",
    "\n",
    "    # create TorchModel\n",
    "    lume_model = TorchModel(\n",
    "        model=\"model/model.pt\",\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        input_transformers=[input_sim_to_nn],\n",
    "        output_transformers=[output_sim_to_nn],\n",
    "    )\n",
    "    # wrap in TorchModule\n",
    "    lume_module = TorchModule(\n",
    "        model=lume_model,\n",
    "        input_order=lume_model.input_names,\n",
    "        output_order=lume_model.output_names,\n",
    "    )\n",
    "\n",
    "    # Log basic parameters\n",
    "    mlflow.log_param(\"model_path\", \"model/model.pt\")\n",
    "    mlflow.log_param(\"input_transformer\", str(type(input_sim_to_nn)))\n",
    "    mlflow.log_param(\"output_transformer\", str(type(output_sim_to_nn)))\n",
    "    mlflow.log_param(\"input_vars\", input_variables)\n",
    "    mlflow.log_param(\"output_vars\", output_variables)\n",
    "\n",
    "    # Log YAML files\n",
    "    mlflow.log_artifact(\"model/sim_variables.yml\")\n",
    "    mlflow.log_artifact(\"model/sim_model.yml\")\n",
    "    mlflow.log_artifact(\"model/sim_module.yml\")\n",
    "\n",
    "    # load example data and calculate predictions\n",
    "    inputs_small = torch.load(\"info/inputs_small.pt\")\n",
    "    outputs_small = torch.load(\"info/outputs_small.pt\")\n",
    "    with torch.no_grad():\n",
    "        predictions = lume_module(inputs_small)\n",
    "\n",
    "    # Log performance metric\n",
    "    mae = torch.mean(torch.abs(predictions - outputs_small)).item()\n",
    "    mlflow.log_metric(\"mean_absolute_error\", mae)\n",
    "\n",
    "    # Plot and save\n",
    "    nrows, ncols = 3, 2\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 15))\n",
    "    for i, output_name in enumerate(lume_module.output_order):\n",
    "        ax_i = ax[i // ncols, i % ncols]\n",
    "        if i < outputs_small.shape[1]:\n",
    "            sort_idx = torch.argsort(outputs_small[:, i])\n",
    "            x_axis = torch.arange(outputs_small.shape[0])\n",
    "            ax_i.plot(x_axis, outputs_small[sort_idx, i], \"C0x\", label=\"outputs\")\n",
    "            ax_i.plot(x_axis, predictions[sort_idx, i], \"C1x\", label=\"predictions\")\n",
    "            ax_i.legend()\n",
    "            ax_i.set_title(output_name)\n",
    "    ax[-1, -1].axis('off')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plot_path = \"comparison_plot_lume.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
